{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda\\lib\\site-packages\\statsmodels\\compat\\pandas.py:23: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  data_klasses = (pandas.Series, pandas.DataFrame, pandas.Panel)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import math "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lets read the example dataset which contains the GRE score, GPA and rank of students and whether they are accepted or not at the university (admit column)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "students = pd.read_csv(\"https://stats.idre.ucla.edu/stat/data/binary.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**we move the admit to the last column in the dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0 = students.pop('admit')\n",
    "students['admit']=df0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For writing a decision tree we need some functions. The first function (left_right) devides the dataset into left and right data\n",
    "according to the split criteria (splt_crt) and the column number of the data (cl_num) which we want to split.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def left_right(data,cl_num,splt_crt):\n",
    "    Y = data.iloc[:,-1]\n",
    "    X = data.iloc[:,0:len(data)-1]\n",
    "    left = X[X.iloc[:,cl_num] < splt_crt]\n",
    "    right = X[X.iloc[:,cl_num] >= splt_crt]\n",
    "    return left, right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We also need to define a split criteria and select what feature should split first in our tree. `Info_gain` function returns column number (col_min)\n",
    "of the feature that should be splitted first and the split criteria for that feature according to the Gini Index. It also returns \n",
    "the Impurity weight (weight_Imp) which helps us to decide after what depth we do not need more split and spliting does not add too much to our information (one of the tresholds to terminate the devision).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Info_gain(data):\n",
    "    gini = np.zeros((len(data),len(data.columns)-1))\n",
    "    w_Imp = np.zeros((len(data),len(data.columns)-1))\n",
    "    for i in range(len(data.columns)-1):\n",
    "        for j in range(len(data)):\n",
    "            splt_crt = data.iloc[j,i]\n",
    "            left, right = left_right(data,i,splt_crt)\n",
    "            Y_l = left.iloc[:,-1]\n",
    "            Y_r = right.iloc[:,-1]\n",
    "            if len(Y_l)== 0:\n",
    "                left_Imp = 0\n",
    "            else:\n",
    "                left_Imp = 1-(sum(Y_l)/len(Y_l))**2-((len(Y_l)-sum(Y_l))/len(Y_l))**2   ## left Impurity\n",
    "            if len(Y_r)== 0:\n",
    "                right_Imp = 0\n",
    "            else:\n",
    "                right_Imp = 1-(sum(Y_r)/len(Y_r))**2-((len(Y_r)-sum(Y_r))/len(Y_r))**2  ## right Impurity\n",
    "            Imp = 1-(len(Y_l)/len(data))**2-(len(Y_r)/len(data))**2                     ## Impurity\n",
    "            gain = Imp-(len(Y_l)/len(data))*left_Imp-(len(Y_r)/len(data))*right_Imp     ## Information Gain\n",
    "            w_Imp[j,i] = (len(Y_l)+len(Y_r))/len(data)*gain                             ## Impurity Weight\n",
    "            gini[j,i] = (len(Y_l)/len(data))*left_Imp+(len(Y_r)/len(data))*right_Imp    ## Gini Index\n",
    "    row_min = np.argmin(np.min(gini, axis=1))\n",
    "    col_min = np.argmin(np.min(gini, axis=0))\n",
    "    splt_crt = data.iloc[row_min,col_min]\n",
    "    weight_Imp = w_Imp[row_min,col_min]\n",
    "    return col_min, splt_crt, weight_Imp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**As we need the output of the decision tree, we define a function that terminates the tree branches and print the admit\n",
    "result (0 or 1)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def terminate(group):\n",
    "    max_value_count = list(group.iloc[:,-1].mode())\n",
    "    return max_value_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split function is the main function of the decision tree algorithm. We recursively split the dataset to right and left branches according to the split criteria (Gini Index or Entropy). This function needs the dataset that we want to create a decision tree for it `(features)`, the minimum row numbers to define the minimum required observation in each branch that we are allowded to split `(min_rows)`, maximum depth of the tree `(max_depth)`, a threshold `(tresh)` for Impurity weight (If the Impurity weight is less than threshold we terminate the tree because spliting does not add any information for us), `depth` which is the numerator for how deep we go in the tree and the `result` which the function returns(contains data column index(the data that is splitted), split criteria, depth of the current branch, the branch name (left or right). For the end of a tree branch we print -1 and -1 which means None value, depth, left or right branch and admit value (1 or 0))**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(features,min_rows,max_depth,tresh,depth,result):\n",
    "    cl_num,splt_crt,weight_Imp= Info_gain(features)\n",
    "    left,right = left_right(features,cl_num,splt_crt)\n",
    "    if depth >= max_depth:\n",
    "        result.append([-1,-1,depth,'left',np.array(terminate(left))[0]])\n",
    "        result.append([-1,-1,depth,'right',np.array(terminate(right))[0]])\n",
    "        return\n",
    "    if len(left)<= min_rows or weight_Imp <= tresh:\n",
    "        result.append([-1,-1,depth,'left',np.array(terminate(left))[0]])\n",
    "    else:\n",
    "        result.append([Info_gain(left)[0],Info_gain(left)[1],depth,'left'])\n",
    "        split(left,min_rows,max_depth,tresh,depth+1,result)\n",
    "    if len(right) <= min_rows or weight_Imp <= tresh:\n",
    "        result.append([-1,-1,depth,'right',np.array(terminate(right))[0]])\n",
    "    else:\n",
    "        result.append([Info_gain(right)[0],Info_gain(right)[1],depth,'right'])\n",
    "        split(right,min_rows,max_depth,tresh,depth+1,result)            \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To predict the admit results for test data we need to define a predict function for our decision tree. I saved the decision tree in the `split` function as a list of branches that each row of the list is a branch (Materialized Path format). In the predict function I first read the first row of the list which is the first node with depth=0 and compare the test data with its criteria, If the test data feature value is less than that criteria we need to move to the left branch with depth 1, In that case we do not need all branches that are made by right branch with depth=1. This helps to get rid of unneccessary branches, in the next step we compare the data value to the second node condition (second on the left here) and if it is greater than the criteria for example we need to move to the depth 2 (depth+1) and left branch. Then we remove all branches that are made with the left node at depth=2. This approach continues untill we reach to the end of the decision tree and finally return the prediction value of that node.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data,tree,row):\n",
    "    depth = 0\n",
    "    n = len(tree)-1\n",
    "    d = max(tree['Depth'])\n",
    "    row_b=0\n",
    "    while (tree['Index'][row_b])>=0:\n",
    "        if data.loc[row][int(tree['Index'][row_b])] < tree['Criteria'][row_b]:\n",
    "            depth = depth+1\n",
    "            row_b = int(tree[(tree['Depth']==depth) & (tree['branch']=='left')].index.tolist()[0])\n",
    "            row_end =int(tree[(tree['Depth']==depth) & (tree['branch']=='right')].index.tolist()[0])\n",
    "            tree = tree.loc[row_b:row_end,:]\n",
    "            prd = tree['value'][row_b]\n",
    "            \n",
    "        else:\n",
    "            depth = depth+1\n",
    "            row_b = int(tree[(tree['Depth']==depth) & (tree['branch']=='right')].index.tolist()[0])\n",
    "            tree = tree.loc[row_b:,:]\n",
    "            prd = tree['value'][row_b]\n",
    "            \n",
    "    return prd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lets devide our dataset to test and train group**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = students.sample(frac=0.2,random_state=4)\n",
    "train_mask = pd.Series(True, index=students.index)\n",
    "train_mask[test.index] = False\n",
    "train = students[train_mask].reset_index()\n",
    "del train['index']\n",
    "test = test.reset_index()\n",
    "del test['index']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Decision tree result With Gini Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lets create a decision tree with the train dataset with maximum depth = 5**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = [[Info_gain(train)[0],Info_gain(train)[1],0,'left']]\n",
    "tree = split(train,10,5,0,1,res)\n",
    "df = pd.DataFrame(tree,columns=['Index','Criteria','Depth','branch','value'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lets predict the train admit values with our decision tree (we want this to check for overfitting by comparing with the test prediction)** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "prd_train = np.zeros((len(train),3))\n",
    "for row in range(len(train)):\n",
    "    prd_train[row,0] = predict(train,df,row)\n",
    "    prd_train[row,1] = train['admit'][row]\n",
    "    prd_train[row,2] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lets create the confusion matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>admit</th>\n",
       "      <th>0.0</th>\n",
       "      <th>1.0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Estimate</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0.0</td>\n",
       "      <td>210</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "admit     0.0  1.0\n",
       "Estimate          \n",
       "0.0       210   77\n",
       "1.0         7   26"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prd_admt_train = pd.DataFrame(prd_train,columns=['Estimate','admit','count'])\n",
    "out_counts_train = prd_admt_train.groupby(['Estimate', 'admit'])['count'].count()\n",
    "out_counts_train.unstack()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lets see the accuracy of the decision tree for train data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7375"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(out_counts_train[0][0]+out_counts_train[1][1])/sum(out_counts_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now, we predict the admit values with test dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "prd_test = np.zeros((len(test),3))\n",
    "for row in range(len(test)):\n",
    "    prd_test[row,0] = predict(test,df,row)\n",
    "    prd_test[row,1] = test['admit'][row]\n",
    "    prd_test[row,2] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Confusion Matrix for test dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>admit</th>\n",
       "      <th>0.0</th>\n",
       "      <th>1.0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>estimate</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0.0</td>\n",
       "      <td>52</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "admit     0.0  1.0\n",
       "estimate          \n",
       "0.0        52   19\n",
       "1.0         4    5"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prd_admt_test = pd.DataFrame(prd_test,columns=['estimate','admit','count'])\n",
    "out_counts_test = prd_admt_test.groupby(['estimate', 'admit'])['count'].count()\n",
    "out_counts_test.unstack()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The accuracy for the test dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7125"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(out_counts_test[0][0]+out_counts_test[1][1])/sum(out_counts_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**It seems that the model does a little bit overfit and it returns high accuracy**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Decision tree result With Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lets define Info_gain function for entropy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Info_gain_entropy(data):\n",
    "    entropy = np.zeros((len(data),len(data.columns)-1))\n",
    "    for i in range(len(data.columns)-1):\n",
    "        for j in range(len(data)):\n",
    "            splt_crt = data.iloc[j,i]\n",
    "            left, right = left_right(data,i,splt_crt)\n",
    "            Y_l = left.iloc[:,-1]\n",
    "            Y_r = right.iloc[:,-1]\n",
    "            if len(Y_l)== 0:\n",
    "                left_Imp = 0\n",
    "            else:\n",
    "                p_l = (sum(Y_l)/len(Y_l))\n",
    "                if p_l==0 or p_l==1:\n",
    "                    a=0\n",
    "                    b=0\n",
    "                else:\n",
    "                    a = np.log2(p_l)\n",
    "                    b = np.log2(1-p_l)\n",
    "                left_Imp = -float(p_l)*float(a)-(1-float(p_l))*float(b)                   ## left Impurity\n",
    "            if len(Y_r)== 0:\n",
    "                right_Imp = 0\n",
    "            else:\n",
    "                p_r = sum(Y_r)/len(Y_r)\n",
    "                if p_r==0 or p_r==1:\n",
    "                    a=0\n",
    "                    b=0\n",
    "                else:\n",
    "                    a = np.log2(p_r)\n",
    "                    b = np.log2(1-p_r)\n",
    "                right_Imp = -p_r*a-(1-p_r)*b                                              ## right Impurity\n",
    "            entropy[j,i] = (len(Y_l)/len(data))*left_Imp+(len(Y_r)/len(data))*right_Imp   ## Entropy\n",
    "    row_min = np.argmin(np.min(entropy, axis=1))\n",
    "    col_min = np.argmin(np.min(entropy, axis=0))\n",
    "    splt_crt = data.iloc[row_min,col_min]\n",
    "    return col_min, splt_crt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split function for entropy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_entropy(features,min_rows,max_depth,depth,result):\n",
    "    cl_num,splt_crt= Info_gain_entropy(features)\n",
    "    left,right = left_right(features,cl_num,splt_crt)\n",
    "    if depth >= max_depth:\n",
    "        result.append([-1,-1,depth,'left',np.array(terminate(left))[0]])\n",
    "        result.append([-1,-1,depth,'right',np.array(terminate(right))[0]])\n",
    "        return\n",
    "    if len(left)<= min_rows:\n",
    "        result.append([-1,-1,depth,'left',np.array(terminate(left))[0]])\n",
    "    else:\n",
    "        result.append([Info_gain_entropy(left)[0],Info_gain_entropy(left)[1],depth,'left'])\n",
    "        split_entropy(left,min_rows,max_depth,depth+1,result)\n",
    "    if len(right) <= min_rows:\n",
    "        result.append([-1,-1,depth,'right',np.array(terminate(right))[0]])\n",
    "    else:\n",
    "        result.append([Info_gain_entropy(right)[0],Info_gain_entropy(right)[1],depth,'right'])\n",
    "        split_entropy(right,min_rows,max_depth,depth+1,result)            \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Decision tree for entropy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = [[Info_gain_entropy(train)[0],Info_gain_entropy(train)[1],0,'left']]\n",
    "tree_entropy = split_entropy(train,10,5,1,res)\n",
    "df_entropy = pd.DataFrame(tree_entropy,columns=['Index','Criteria','Depth','branch','value'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train prediction for entropy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "prd_train = np.zeros((len(train),3))\n",
    "for row in range(len(train)):\n",
    "    prd_train[row,0] = predict(train,df_entropy,row)\n",
    "    prd_train[row,1] = train['admit'][row]\n",
    "    prd_train[row,2] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Confusion Matrix for train entropy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>admit</th>\n",
       "      <th>0.0</th>\n",
       "      <th>1.0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Estimate</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0.0</td>\n",
       "      <td>203</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1.0</td>\n",
       "      <td>14</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "admit     0.0  1.0\n",
       "Estimate          \n",
       "0.0       203   59\n",
       "1.0        14   44"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prd_admt_train = pd.DataFrame(prd_train,columns=['Estimate','admit','count'])\n",
    "out_counts_train = prd_admt_train.groupby(['Estimate', 'admit'])['count'].count()\n",
    "out_counts_train.unstack()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train accuracy by entropy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.771875"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(out_counts_train[0][0]+out_counts_train[1][1])/sum(out_counts_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test prediction by entropy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "prd_test = np.zeros((len(test),3))\n",
    "for row in range(len(test)):\n",
    "    prd_test[row,0] = predict(test,df_entropy,row)\n",
    "    prd_test[row,1] = test['admit'][row]\n",
    "    prd_test[row,2] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test confusion matrix for entropy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>admit</th>\n",
       "      <th>0.0</th>\n",
       "      <th>1.0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>estimate</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0.0</td>\n",
       "      <td>50</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "admit     0.0  1.0\n",
       "estimate          \n",
       "0.0        50   19\n",
       "1.0         6    5"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prd_admt_test = pd.DataFrame(prd_test,columns=['estimate','admit','count'])\n",
    "out_counts_test = prd_admt_test.groupby(['estimate', 'admit'])['count'].count()\n",
    "out_counts_test.unstack()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test accuracy ny emtropy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6875"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(out_counts_test[0][0]+out_counts_test[1][1])/sum(out_counts_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training accuracy for entropy is higher than gini index but the test accuracy for it is less**"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "PS1.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
