{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ahmad Hojatimalekshah"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collaborate: Arash Modaresi, Amir Kazemzadeh, Ali Nazari"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "czVETi-U8I0h"
   },
   "source": [
    "# CS534 Homework 2\n",
    "\n",
    "Put your homework in the directory with your name. Please mentionin this file the names of any students with whom you collaborated. If you didn't collaborate with anyone, mark your collaborators as \"None.\" Remember, your goal is to communicate. Full credit will be given only to correct solutions which are described clearly. Convoluted and obtuse descriptions will receive low marks. To complete your homework, you may ONLY consult the following material:\n",
    "\n",
    "lecture slides\n",
    "course notes you or others took during lecture.\n",
    "the required text (CLRS)\n",
    "websites that may clarify the concepts covered in the material but do not in any way provide complete solutions to the problems.\n",
    "Deadline 3/4/2020\n",
    "\n",
    "Please provide an answer to the following question: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "byP0-Jsv8I0i"
   },
   "source": [
    "# Question 1 (15 pts)\n",
    "Implement the fit and predict procedures for the logistic regression (scikit is not allowed) with norm 2 regularization function (and Lambda parameter).\n",
    "Use as the imput parameters of the gradient ascent the maximum number of iterations (just a constant e.g 100) and the learning factor (e.g. 0.01).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import numpy as np\n",
    "import math \n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split , StratifiedKFold,KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We use iris data for the logistic regression. For creating binary data, we set setosa Species to 0 and the others as 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SepalLength</th>\n",
       "      <th>SepalWidth</th>\n",
       "      <th>PetalLength</th>\n",
       "      <th>PetalWidth</th>\n",
       "      <th>Species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SepalLength  SepalWidth  PetalLength  PetalWidth  Species\n",
       "0          5.1         3.5          1.4         0.2        0\n",
       "1          4.9         3.0          1.4         0.2        0\n",
       "2          4.7         3.2          1.3         0.2        0\n",
       "3          4.6         3.1          1.5         0.2        0\n",
       "4          5.0         3.6          1.4         0.2        0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_d = sm.datasets.get_rdataset('iris')\n",
    "iris = iris_d.data\n",
    "iris.rename(columns=lambda x: x.replace('.', ''), inplace=True)\n",
    "iris.loc[iris.Species != 'setosa','Species']=1\n",
    "iris.loc[iris.Species == 'setosa','Species']=0\n",
    "iris.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We split train(70%) and test(30%) data.\n",
    "- y_test and y_train are test and train targets.\n",
    "- x_test and x_train are test and train features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = iris.sample(frac=0.3,random_state=4)\n",
    "train_mask = pd.Series(True, index=iris.index)\n",
    "train_mask[test.index] = False\n",
    "train = iris[train_mask]\n",
    "\n",
    "y_test = test.Species\n",
    "y_train = train.Species\n",
    "\n",
    "x_test = test.drop('Species', axis=1)\n",
    "x_train = train.drop('Species', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We need to standardize the dataset, then we subtract the mean of the feature column from each column and devide the result by the standard deviation of the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_std = (x_train-np.mean(x_train))/np.std(x_train)\n",
    "x_test_std = (x_test-np.mean(x_test))/np.std(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression with l2 norm\n",
    "\n",
    "- X in the function is features\n",
    "- Y is the target (0 or 1)\n",
    "- theta is the coefficients we want to estimate\n",
    "- lrnn_rte is the information or learning rate of the gradient ascent (here 0.01)\n",
    "- num_itr is the number of iteration for ending the gradient ascent optimization \n",
    "- C is the lambda parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_reg_l2(X,Y,lrn_rte,num_itr,C):\n",
    "    X = np.hstack((np.ones((len(X), 1)), X))\n",
    "    theta = np.zeros((X.shape[1], 1))\n",
    "\n",
    "    # loop over number of iterations\n",
    "    for itr in range(num_itr):\n",
    "\n",
    "    # predict probability for each row in the dataset\n",
    "        predictions = (1 / (1 + np.exp(-np.dot(X, theta))))\n",
    "        \n",
    "    # calculate the errors\n",
    "        errors = (np.array([Y])).T - predictions\n",
    "\n",
    "    # loop over each weight coefficient\n",
    "        for i in range(len(theta)):\n",
    "            \n",
    "    # derivation of log-likelihood function + l2 regularization        \n",
    "            drvt = np.dot((errors).T, X[:,i])\n",
    "            if i!=0:\n",
    "                drvt -= 2 * C * theta[i]\n",
    "                \n",
    "    # Gradient Ascent update          \n",
    "            theta[i] += lrn_rte * drvt\n",
    "        \n",
    "    # compute the log-likelihood\n",
    "        ll = np.sum(((np.array([Y])).T-1)*np.dot(X, theta) - np.log(1 + np.exp(-np.dot(X, theta))))- (C * np.sum(theta[1:]**2))\n",
    "        \n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = logistic_reg_l2(x_train_std, y_train,1e-2,100,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For finding the accuracy we need to add ones column to the train and test feature data for intercept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test  = np.hstack((np.ones((len(x_test_std), 1)), x_test_std))\n",
    "X_train  = np.hstack((np.ones((len(x_train_std), 1)), x_train_std))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Probabilities (sigmoid) greater than 0.5 are taken as 1 and probabilities less than 0.5 are taken as 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions  = ((1 / (1 + np.exp(-np.dot(X_test, theta)))).flatten()>0.5)\n",
    "train_predictions = ((1 / (1 + np.exp(-np.dot(X_train, theta)))).flatten()>0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prd_train = pd.DataFrame({'estimate':(train_predictions).astype('int64'), 'real':y_train,'count':np.ones((len(y_train)))})\n",
    "prd_admt_train = pd.DataFrame(prd_train,columns=['estimate','real','count'])\n",
    "out_counts_train = prd_admt_train.groupby(['estimate', 'real'])['count'].count()\n",
    "# out_counts_test.unstack()\n",
    "(out_counts_train[0][0]+out_counts_train[1][1])/sum(out_counts_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9777777777777777"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prd_test = pd.DataFrame({'estimate':(test_predictions).astype('int64'), 'real':y_test,'count':np.ones((len(y_test)))})\n",
    "prd_admt_test = pd.DataFrame(prd_test,columns=['estimate','real','count'])\n",
    "out_counts_test = prd_admt_test.groupby(['estimate', 'real'])['count'].count()\n",
    "# out_counts_test.unstack()\n",
    "(out_counts_test[0][0]+out_counts_test[1][1])/sum(out_counts_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lyWdLh3B8I0l"
   },
   "source": [
    "# Question 2 (20 pts)\n",
    "Use the iris dataset (just the binary class Iris Setosa vs others), the K-fold cross validation, metrics(accuracy, precision, recall, F1-score) and the logistic regression (Question1) with L2 regularization.\n",
    "You can use scikit.\n",
    "Please estimate the best parameter C(the inverse of lambda) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We set y value for all species rather than Setosa equal 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target \n",
    "y[y == 2]=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, X_test, y, y_test = train_test_split(X, y, test_size=0.3, random_state=4)\n",
    "sc = StandardScaler()\n",
    "X_test_std = sc.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression fit by l2 and the results (accuracy, precision, recall, F1_score)\n",
    "#### We use K-fold cross validation with number of splits equal 5\n",
    "#### We also standardize train and tests "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- C : the inverse of the lambda\n",
    "- n_splits : number of splits for K-Fold cross validation\n",
    "- ind : list of test and train index splitted by K-Fold\n",
    "- df2 : result dataframe containing the C, fold number, train accuracy, test accuracy, recall, precision, F1_score\n",
    "- X_train_std: standardized train features\n",
    "- X_test_std: standardized test features\n",
    "- y_train: train targets (0,1)\n",
    "- y_test: test targets (0,1)\n",
    "- y_train_predict: train prediction\n",
    "- y_test_predict: test prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining \"C\" values\n",
    "C = np.linspace(0.0001,0.05,50)\n",
    "# C = [0.0001,0.,0.0005 ,0.001]\n",
    "\n",
    "# number of splits for K-Fold cross validation\n",
    "n_splits= 5\n",
    "\n",
    "# Defining a K-Fold cross validation\n",
    "skf = KFold(n_splits=n_splits,random_state=4,shuffle=True )\n",
    "\n",
    "# Train and validation data index\n",
    "ind = list(skf.split(X, y))\n",
    "\n",
    "# Empty DataFrame for results\n",
    "df2 = pd.DataFrame()\n",
    "\n",
    "# loop over \"C\" and different folds\n",
    "for i in C:\n",
    "    \n",
    "    # Creating logistic regression fit + l2 regularization\n",
    "    clf = LogisticRegression(penalty='l2', C=i, solver='liblinear')\n",
    "    \n",
    "    # Loop over the folds\n",
    "    for j in range(0,n_splits):\n",
    "        # Train and test data for each fold\n",
    "        X_train = X[ind[j][0]]\n",
    "        y_train = y[ind[j][0]]\n",
    "        X_valid = X[ind[j][1]]\n",
    "        y_valid = y[ind[j][1]]\n",
    "        \n",
    "        # Standardize the train and test data\n",
    "        sc = StandardScaler()\n",
    "        X_train_std = sc.fit_transform(X_train)\n",
    "        X_valid_std = sc.transform(X_valid)\n",
    "        \n",
    "        # Fitting the logistic regression\n",
    "        clf.fit(X_train_std, y_train)\n",
    "        \n",
    "        # train and test prediction\n",
    "        y_train_predict = clf.predict(X_train_std)\n",
    "        y_valid_predict = clf.predict(X_valid_std)\n",
    "        \n",
    "        # Writing the results as a dataframe \n",
    "        df1 = pd.DataFrame({'Fold':[j+1],'C':[i], 'Train Accuracy':[clf.score(X_train_std, y_train)],\n",
    "                            'Validation Accuracy':[clf.score(X_valid_std, y_valid)], 'Recall':[recall_score(y_valid, y_valid_predict)],\n",
    "                            'Precision':[precision_score(y_valid, y_valid_predict)],'F1_score':[f1_score(y_valid, y_valid_predict)]})\n",
    "        \n",
    "        df2 = pd.concat([df1, df2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fold</th>\n",
       "      <th>C</th>\n",
       "      <th>Train Accuracy</th>\n",
       "      <th>Validation Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>0.048982</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>0.048982</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0.048982</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0.048982</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.048982</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>0.047963</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>0.047963</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0.047963</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0.047963</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.047963</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>0.046945</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>0.046945</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0.046945</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0.046945</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.046945</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>0.045927</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>0.045927</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0.045927</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0.045927</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.045927</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>0.044908</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>0.044908</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0.044908</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0.044908</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.044908</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>0.005192</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>0.005192</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0.005192</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0.005192</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.005192</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>0.004173</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>0.004173</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0.004173</td>\n",
       "      <td>0.988095</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0.004173</td>\n",
       "      <td>0.988095</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.004173</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>0.003155</td>\n",
       "      <td>0.988095</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>0.003155</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0.003155</td>\n",
       "      <td>0.976190</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0.003155</td>\n",
       "      <td>0.976190</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.003155</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>0.002137</td>\n",
       "      <td>0.988095</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>0.9375</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.967742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>0.002137</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0.002137</td>\n",
       "      <td>0.976190</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0.002137</td>\n",
       "      <td>0.976190</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.002137</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>0.001118</td>\n",
       "      <td>0.988095</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>0.9375</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.967742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>0.001118</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0.001118</td>\n",
       "      <td>0.976190</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0.001118</td>\n",
       "      <td>0.976190</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.001118</td>\n",
       "      <td>0.988095</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>0.9375</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.967742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.988095</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>0.9375</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.967742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.976190</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.976190</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.988095</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>0.9375</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.967742</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Fold         C  Train Accuracy  Validation Accuracy  Recall  Precision  \\\n",
       "0      5  0.050000        1.000000             1.000000  1.0000        1.0   \n",
       "0      4  0.050000        1.000000             1.000000  1.0000        1.0   \n",
       "0      3  0.050000        1.000000             1.000000  1.0000        1.0   \n",
       "0      2  0.050000        1.000000             1.000000  1.0000        1.0   \n",
       "0      1  0.050000        1.000000             1.000000  1.0000        1.0   \n",
       "0      5  0.048982        1.000000             1.000000  1.0000        1.0   \n",
       "0      4  0.048982        1.000000             1.000000  1.0000        1.0   \n",
       "0      3  0.048982        1.000000             1.000000  1.0000        1.0   \n",
       "0      2  0.048982        1.000000             1.000000  1.0000        1.0   \n",
       "0      1  0.048982        1.000000             1.000000  1.0000        1.0   \n",
       "0      5  0.047963        1.000000             1.000000  1.0000        1.0   \n",
       "0      4  0.047963        1.000000             1.000000  1.0000        1.0   \n",
       "0      3  0.047963        1.000000             1.000000  1.0000        1.0   \n",
       "0      2  0.047963        1.000000             1.000000  1.0000        1.0   \n",
       "0      1  0.047963        1.000000             1.000000  1.0000        1.0   \n",
       "0      5  0.046945        1.000000             1.000000  1.0000        1.0   \n",
       "0      4  0.046945        1.000000             1.000000  1.0000        1.0   \n",
       "0      3  0.046945        1.000000             1.000000  1.0000        1.0   \n",
       "0      2  0.046945        1.000000             1.000000  1.0000        1.0   \n",
       "0      1  0.046945        1.000000             1.000000  1.0000        1.0   \n",
       "0      5  0.045927        1.000000             1.000000  1.0000        1.0   \n",
       "0      4  0.045927        1.000000             1.000000  1.0000        1.0   \n",
       "0      3  0.045927        1.000000             1.000000  1.0000        1.0   \n",
       "0      2  0.045927        1.000000             1.000000  1.0000        1.0   \n",
       "0      1  0.045927        1.000000             1.000000  1.0000        1.0   \n",
       "0      5  0.044908        1.000000             1.000000  1.0000        1.0   \n",
       "0      4  0.044908        1.000000             1.000000  1.0000        1.0   \n",
       "0      3  0.044908        1.000000             1.000000  1.0000        1.0   \n",
       "0      2  0.044908        1.000000             1.000000  1.0000        1.0   \n",
       "0      1  0.044908        1.000000             1.000000  1.0000        1.0   \n",
       "..   ...       ...             ...                  ...     ...        ...   \n",
       "0      5  0.005192        1.000000             1.000000  1.0000        1.0   \n",
       "0      4  0.005192        1.000000             1.000000  1.0000        1.0   \n",
       "0      3  0.005192        1.000000             1.000000  1.0000        1.0   \n",
       "0      2  0.005192        1.000000             1.000000  1.0000        1.0   \n",
       "0      1  0.005192        1.000000             1.000000  1.0000        1.0   \n",
       "0      5  0.004173        1.000000             1.000000  1.0000        1.0   \n",
       "0      4  0.004173        1.000000             1.000000  1.0000        1.0   \n",
       "0      3  0.004173        0.988095             1.000000  1.0000        1.0   \n",
       "0      2  0.004173        0.988095             1.000000  1.0000        1.0   \n",
       "0      1  0.004173        1.000000             1.000000  1.0000        1.0   \n",
       "0      5  0.003155        0.988095             1.000000  1.0000        1.0   \n",
       "0      4  0.003155        1.000000             1.000000  1.0000        1.0   \n",
       "0      3  0.003155        0.976190             1.000000  1.0000        1.0   \n",
       "0      2  0.003155        0.976190             1.000000  1.0000        1.0   \n",
       "0      1  0.003155        1.000000             1.000000  1.0000        1.0   \n",
       "0      5  0.002137        0.988095             0.952381  0.9375        1.0   \n",
       "0      4  0.002137        1.000000             1.000000  1.0000        1.0   \n",
       "0      3  0.002137        0.976190             1.000000  1.0000        1.0   \n",
       "0      2  0.002137        0.976190             1.000000  1.0000        1.0   \n",
       "0      1  0.002137        1.000000             1.000000  1.0000        1.0   \n",
       "0      5  0.001118        0.988095             0.952381  0.9375        1.0   \n",
       "0      4  0.001118        1.000000             1.000000  1.0000        1.0   \n",
       "0      3  0.001118        0.976190             1.000000  1.0000        1.0   \n",
       "0      2  0.001118        0.976190             1.000000  1.0000        1.0   \n",
       "0      1  0.001118        0.988095             0.952381  0.9375        1.0   \n",
       "0      5  0.000100        0.988095             0.952381  0.9375        1.0   \n",
       "0      4  0.000100        1.000000             1.000000  1.0000        1.0   \n",
       "0      3  0.000100        0.976190             1.000000  1.0000        1.0   \n",
       "0      2  0.000100        0.976190             1.000000  1.0000        1.0   \n",
       "0      1  0.000100        0.988095             0.952381  0.9375        1.0   \n",
       "\n",
       "    F1_score  \n",
       "0   1.000000  \n",
       "0   1.000000  \n",
       "0   1.000000  \n",
       "0   1.000000  \n",
       "0   1.000000  \n",
       "0   1.000000  \n",
       "0   1.000000  \n",
       "0   1.000000  \n",
       "0   1.000000  \n",
       "0   1.000000  \n",
       "0   1.000000  \n",
       "0   1.000000  \n",
       "0   1.000000  \n",
       "0   1.000000  \n",
       "0   1.000000  \n",
       "0   1.000000  \n",
       "0   1.000000  \n",
       "0   1.000000  \n",
       "0   1.000000  \n",
       "0   1.000000  \n",
       "0   1.000000  \n",
       "0   1.000000  \n",
       "0   1.000000  \n",
       "0   1.000000  \n",
       "0   1.000000  \n",
       "0   1.000000  \n",
       "0   1.000000  \n",
       "0   1.000000  \n",
       "0   1.000000  \n",
       "0   1.000000  \n",
       "..       ...  \n",
       "0   1.000000  \n",
       "0   1.000000  \n",
       "0   1.000000  \n",
       "0   1.000000  \n",
       "0   1.000000  \n",
       "0   1.000000  \n",
       "0   1.000000  \n",
       "0   1.000000  \n",
       "0   1.000000  \n",
       "0   1.000000  \n",
       "0   1.000000  \n",
       "0   1.000000  \n",
       "0   1.000000  \n",
       "0   1.000000  \n",
       "0   1.000000  \n",
       "0   0.967742  \n",
       "0   1.000000  \n",
       "0   1.000000  \n",
       "0   1.000000  \n",
       "0   1.000000  \n",
       "0   0.967742  \n",
       "0   1.000000  \n",
       "0   1.000000  \n",
       "0   1.000000  \n",
       "0   0.967742  \n",
       "0   0.967742  \n",
       "0   1.000000  \n",
       "0   1.000000  \n",
       "0   1.000000  \n",
       "0   0.967742  \n",
       "\n",
       "[250 rows x 7 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the mean train and test accuracy by different \"C\"s "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We group the result by \"C\" value and take the mean of the test and train accuracy for each group. Then. we plot those by different \"C\" values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0xc6f9908>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(C, df2.groupby(['C'])['Train Accuracy'].mean(), 'b')\n",
    "plt.plot(C, df2.groupby(['C'])['Validation Accuracy'].mean(), '--r')\n",
    "plt.legend((\"Train Accuracy\", \"Validation Accuracy\"),fontsize=24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We chose C, where the validation and train started to become 1. \n",
    "#### It seems C=0.005 is the optimum C for the logistic regression. However, values less than C in the interval where the test accuracy started to become greater than train (both have high accuracy) can also be considered the optimum C."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets see the prediction for the test dataset by C=0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = StandardScaler()\n",
    "X_train_std = sc.fit_transform(X)\n",
    "# logisitc regression fit by C = 0.005\n",
    "clf = LogisticRegression(penalty='l2', C=0.005, solver='liblinear')\n",
    "clf.fit(X_train_std, y)\n",
    "# test accuracy\n",
    "clf.score(X_test_std, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "PS2.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
